•	pandas: for data manipulation
•	numpy: for numerical operations
•	matplotlib and seaborn: for data visualization
•	scikit-learn: for machine learning algorithms
•	surprise: for building recommendation systems
•	tensorflow or keras (optional): for deep learning approaches (if you want to build a neural network-based model)
pip install pandas numpy matplotlib seaborn scikit-learn surprise
# Load the MovieLens dataset
ratings = pd.read_csv('ml-100k/u.data', sep='\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])
movies = pd.read_csv('ml-100k/u.item', sep='|', names=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL'], encoding='latin-1')

# Merge ratings with movie titles
movie_data = pd.merge(ratings, movies[['movie_id', 'movie_title']], on='movie_id')
# Check the first few rows of the dataset
print(movie_data.head())

# Number of unique movies and users
print(f"Number of unique movies: {movie_data['movie_id'].nunique()}")
print(f"Number of unique users: {movie_data['user_id'].nunique()}")

# Average rating per movie
average_rating_per_movie = movie_data.groupby('movie_title')['rating'].mean().sort_values(ascending=False)
print(average_rating_per_movie.head())

# Distribution of ratings
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='rating', data=movie_data)
plt.title('Distribution of Ratings')
plt.show()
from surprise import Reader, Dataset

# Define the format for the data
reader = Reader(rating_scale=(1, 5))

# Load data into Surprise format
data = Dataset.load_from_df(movie_data[['user_id', 'movie_id', 'rating']], reader)

# Split the data into training and testing sets
from surprise.model_selection import train_test_split
trainset, testset = train_test_split(data, test_size=0.2)
from surprise import KNNBasic
from surprise import accuracy

# Use KNN for collaborative filtering
sim_options = {
    'name': 'cosine',
    'user_based': True  # Use user-based collaborative filtering
}

model = KNNBasic(sim_options=sim_options)

# Train the model
model.fit(trainset)

# Test the model
predictions = model.test(testset)

# Evaluate the model
rmse = accuracy.rmse(predictions)
print(f"RMSE: {rmse}")
# Predict rating for a specific user-item pair
user_id = 1  # Example user ID
movie_id = 50  # Example movie ID (e.g., Star Wars)

prediction = model.predict(user_id, movie_id)
print(f"Predicted rating for user {user_id} on movie {movie_id}: {prediction.est}")
from surprise import SVD

# Use SVD for collaborative filtering
model_svd = SVD()

# Train the SVD model
model_svd.fit(trainset)

# Test the SVD model
predictions_svd = model_svd.test(testset)

# Evaluate the model
rmse_svd = accuracy.rmse(predictions_svd)
print(f"RMSE for SVD: {rmse_svd}")
def get_top_n(predictions, n=10):
    # First map the predictions to each user
    top_n = {}
    for uid, iid, true_r, est, _ in predictions:
        if uid not in top_n:
            top_n[uid] = []
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the top N
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

# Get top 10 recommendations for each user
top_n_predictions = get_top_n(predictions_svd, n=10)

# Display recommendations for a specific user
user_id = 1
recommended_movies = top_n_predictions[user_id]

print(f"Top 10 recommended movies for user {user_id}:")
for movie_id, rating in recommended_movies:
    movie_title = movies[movies['movie_id'] == movie_id]['movie_title'].values[0]
    print(f"{movie_title} with predicted rating: {rating:.2f}")
import joblib

# Save the trained SVD model
joblib.dump(model_svd, 'svd_movie_recommendation_model.pkl')
